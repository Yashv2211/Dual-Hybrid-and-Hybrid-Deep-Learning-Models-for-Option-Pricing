{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJWaHtbcfCBw"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4f1OLX-KLlw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyeBGVPf-TlD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk-PoHrA4cEK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from tensorflow.keras.callbacks import History\n",
        "import seaborn as sns\n",
        "import time as time\n",
        "sns.set( color_codes=True)\n",
        "sns.set_style('darkgrid', {'axes.facecolor': '.9'})\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from copy import copy\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as si\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW3bhUKqVXiz"
      },
      "outputs": [],
      "source": [
        "months = np.arange(1, 13, step = 1)\n",
        "print(months)\n",
        "years = np.arange(2006, 2022, step = 1 )\n",
        "print(years)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXeTsf_MW18f"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, date\n",
        "from dateutil.relativedelta import relativedelta, MO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKXtAdgBZkKU"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
      ],
      "metadata": {
        "id": "rRYoAzpvGwjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMS3e-ulbKHB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgIE1DUYOKh-"
      },
      "outputs": [],
      "source": [
        "def bsmcall(S, K, T, r,q, sigma):\n",
        "    d1 = (np.log(S/K) + (r - q + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T)) \n",
        "    d2 = d1 - sigma * np.sqrt(T)\n",
        "    return S * np.exp(-q*T) * si.norm.cdf(d1, 0.0, 1.0) - K * np.exp(-r * T) * si.norm.cdf(d2, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize, leastsq\n",
        "\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "metadata": {
        "id": "y9aji3paF72C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBfvubtDni3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAGpbevhTBPv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score \n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6bZeQUfsruj"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MLP"
      ],
      "metadata": {
        "id": "eD5IDjZHnpBB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGqDaXM0Vv0-"
      },
      "outputs": [],
      "source": [
        "# defining the different models\n",
        "\n",
        "def mlp(x_train,x_test,y_train,y_test,y,vol_train,vol_test,scaler_y,BSM_train,BSM_test,optimalvalues):\n",
        "  from pathlib import Path  \n",
        "\n",
        "  for k in range(1):\n",
        "    mse_test,  mse_BSM_test = np.zeros(5), np.zeros(5)\n",
        "    rmse_test  , rmse_BSM_test = np. zeros (5) , np. zeros (5) \n",
        "    R2_test  , R2_BSM_test = np. zeros (5) , np. zeros (5) \n",
        "    diff_test ,  diff_BSM_test = np.zeros((len(x_test),5)), np.zeros((len(x_test),5))\n",
        "    modelList = [('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5', 'ARIMA RUT.h5', ' VXN RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' VXN RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' VXN RUT.h5 ' ),('vol10 RUT.h5','RUT SPX.h5','vol60 RUT.h5', 'ARIMA RUT.h5', ' VXN RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' VXN RUT.h5 ' ) ]\n",
        "    errorList_csv = [('diff_test_RUT_1.csv',  'diff_BSM_test1.csv' ),('diff_test_RUT_2.csv', 'diff_BSM_test2.csv' ),('diff_test_RUT_3.csv',  'diff_BSM_test3.csv' ),('diff_test_RUT_4.csv', 'diff_BSM_test4.csv' ),('diff_test_RUT_5.csv', 'diff_BSM_test5.csv' )]\n",
        "    scoreList_csv = [('mse_test1.csv','rmse_test1.csv','R2_test1.csv')]\n",
        "    scoreListBSM_csv = [('mse_BSM_test1.csv','rmse_BSM_test1.csv','R2_BSM_test1.csv')]\n",
        "    for i in range(5):\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "      X_train_sc = np.append(X_train, np.resize(vol_train[:,i], (len(vol_train),1)), axis = 1)\n",
        "      X_test_sc = np.append(X_test, np.resize(vol_test[:,i], (len(vol_test),1)), axis = 1)\n",
        "      print(X_test_sc)\n",
        "\n",
        "      model = Sequential()\n",
        "      model.add(layers.Dense(128, input_dim = 5))\n",
        "      model.add(layers.Dense(128, activation = 'relu'))\n",
        "      model.add(layers.Dense(128, activation = 'relu'))\n",
        "      model.add(layers.Dense(1, activation = 'linear'))\n",
        "      model.compile(loss='mse' ,optimizer='adam')\n",
        "      model.summary()\n",
        "      mod = model.fit(X_train_sc, y_train , epochs=500, batch_size=1024, validation_data=(X_test_sc, y_test), verbose = 1, callbacks=[earlyStop])\n",
        "      y_test_hat = np.float64(scaler_y.inverse_transform(model.predict(X_test_sc)))\n",
        "      training_loss = mod.history['loss']\n",
        "      test_loss = mod.history['val_loss']\n",
        "      out = y_test_hat.reshape(y_test_hat.shape[0])\n",
        "      if i == 0:\n",
        "        vol10['MLP']=pd.Series(out)\n",
        "      if i == 1:\n",
        "        vol30['MLP']=pd.Series(out)\n",
        "      if i == 2:\n",
        "        vol60['MLP']=pd.Series(out)\n",
        "      if i == 3:\n",
        "        ARIMA['MLP']=pd.Series(out)\n",
        "      if i == 4:\n",
        "        VXN['MLP']=pd.Series(out)                           \n",
        "                \n",
        "      # Create count of the number of epochs\n",
        "      epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "      # Visualize loss history\n",
        "      plt.plot(epoch_count, training_loss, 'r--')\n",
        "      plt.plot(epoch_count, test_loss, 'b-')\n",
        "      plt.legend(['Training Loss', 'Test Loss'])\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.show()\n",
        " \n",
        "   \n",
        "\n",
        "\n",
        "      mse_test[i] = mean_squared_error(y, y_test_hat)\n",
        "      rmse_test[i] = math.sqrt(mse_test[i])\n",
        "      R2_test[i] = r2_score(y, y_test_hat)\n",
        "      mse_BSM_test[i] = mean_squared_error(y, BSM_test[:,i:i+1])\n",
        "      rmse_BSM_test[i] = math.sqrt(mse_BSM_test[i])\n",
        "      R2_BSM_test[i] = r2_score(y, BSM_test[:,i:i+1])      \n",
        "      model.save(modelList[k][i])\n",
        "\n",
        "\n",
        "\n",
        "    scoreList = [mse_test,rmse_test,R2_test]\n",
        "    scoreListBSM = [mse_BSM_test,rmse_BSM_test,R2_BSM_test]\n",
        "    \n",
        "    for j in range(3):\n",
        "      filepath = Path('/content/drive/MyDrive/RUT_MLP/'+ scoreList_csv[k][j])  \n",
        "      filepath.parent.mkdir(parents=True, exist_ok=True) \n",
        "      df = pd.DataFrame( scoreList [j])\n",
        "      df = df.rename(columns={\"0\": \"vol10\", \"1\": \"vol30\", \"2\": \"vol60\", \"3\": \"ARIMA\", \"4\":\"VXN\"})\n",
        "      df.to_csv(filepath)\n",
        "    for j in range(3):\n",
        "      filepath = Path('/content/drive/MyDrive/RUT_MLP/'+ scoreListBSM_csv[k][j])  \n",
        "      filepath.parent.mkdir(parents=True, exist_ok=True) \n",
        "      df = pd.DataFrame( scoreListBSM [j])\n",
        "      df = df.rename(columns={\"0\": \"vol10\", \"1\": \"vol30\", \"2\": \"vol60\", \"3\": \"ARIMA\", \"4\":\"VXN\"})\n",
        "      df.to_csv(filepath)\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining GridSearchCV parameters for MLP\n",
        "\n"
      ],
      "metadata": {
        "id": "jamZKpeT_scU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calling grid search\n",
        "def mlpgridsearch():\n",
        "  # define search space\n",
        "  space = dict()\n",
        "  space['hidden_layer'] = [(128,128,128),(128,64,128),(64,128,64),(64,64,64)]\n",
        "  space['activation_function'] = ['tanh','relu']\n",
        "  space['optimizer'] = ['SGD', 'Adam']\n",
        "  search = GridSearchCV(mlp, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)"
      ],
      "metadata": {
        "id": "NkU8_DazCVWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN"
      ],
      "metadata": {
        "id": "5hxuSqpznq9_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtXxJ0ilcped"
      },
      "outputs": [],
      "source": [
        "# defining the different models\n",
        "\n",
        "def cnn(x_train,x_test,y_train,y_test,y,vol_train,vol_test,scaler_y,optimalvalues):\n",
        "  from pathlib import Path  \n",
        "\n",
        "  for k in range(1):\n",
        "    mse_test,  mse_BSM_test = np.zeros(5), np.zeros(5)\n",
        "    rmse_test  , rmse_BSM_test = np. zeros (5) , np. zeros (5) \n",
        "    R2_test  , R2_BSM_test = np. zeros (5) , np. zeros (5) \n",
        "    diff_test ,  diff_BSM_test = np.zeros((len(x_test),5)), np.zeros((len(x_test),5))\n",
        "    modelList = [('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5', 'ARIMA RUT.h5', ' VXN RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' VXN RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' VXN RUT.h5 ' ),('vol10 RUT.h5','RUT SPX.h5','vol60 RUT.h5', 'ARIMA RUT.h5', ' VXN RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' VXN RUT.h5 ' ) ]\n",
        "    errorList_csv = [('diff_test_RUT_1.csv',  'diff_BSM_test1.csv' ),('diff_test_RUT_2.csv', 'diff_BSM_test2.csv' ),('diff_test_RUT_3.csv',  'diff_BSM_test3.csv' ),('diff_test_RUT_4.csv', 'diff_BSM_test4.csv' ),('diff_test_RUT_5.csv', 'diff_BSM_test5.csv' )]\n",
        "    scoreList_csv = [('mse_test1.csv','rmse_test1.csv','R2_test1.csv')]\n",
        "    scoreListBSM_csv = [('mse_BSM_test1.csv','rmse_BSM_test1.csv','R2_BSM_test1.csv')]\n",
        "    for i in range(5):\n",
        "\n",
        "      print('Fold', k+1, ',− Model', i+1)\n",
        "      X_train_sc = np.append(X_train, np.resize(vol_train[:,i], (len(vol_train),1)), axis = 1)\n",
        "      X_test_sc = np.append(X_test, np.resize(vol_test[:,i], (len(vol_test),1)), axis = 1)\n",
        "      X_train_size = X_train_sc.shape[0] # number of samples in train set\n",
        "      X_train_time_steps  = X_train_sc.shape[1] # number of features in train set\n",
        "      input_dimension = 1               # each feature is represented by 1 number\n",
        "      X_train_data_reshaped = X_train_sc.reshape(X_train_size,X_train_time_steps,input_dimension)\n",
        "\n",
        "      X_test_size = X_test_sc.shape[0] # number of samples in train set\n",
        "      X_test_time_steps  = X_test_sc.shape[1] # number of features in train set\n",
        "      input_dimension = 1               # each feature is represented by 1 number\n",
        "      X_test_data_reshaped = X_test_sc.reshape(X_test_size,X_test_time_steps,input_dimension)\n",
        "      a=X_train_data_reshaped.shape[0]\n",
        "      n_timesteps = X_train_data_reshaped.shape[1] #13\n",
        "      n_features  = X_train_data_reshaped.shape[2] #1 \n",
        "      print(n_features)\n",
        "\n",
        "\n",
        "      #model = Sequential()\n",
        "      model = keras.Sequential(name=\"model_conv1D\")\n",
        "      #model.add(tf.keras.layers.inputLayer({inputShape: [n_timesteps, n_features]}))\n",
        "      model.add(keras.layers.Conv1D(64, 1,activation='relu',input_shape=(n_timesteps,n_features)))\n",
        "      #model.add(layers.Conv1D(filters=128, kernel_size=2, activation='relu', input_shape=(n_timesteps, n_features)))\n",
        "      model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "      #model.add(layers.Conv1D(filters=4, activation='relu'))\n",
        "      #model.add(layers.MaxPooling1D(pool_size=2))\n",
        "      model.add(layers.Flatten())\n",
        "      \n",
        "      model.add(layers.Dense(64, activation='elu'))\n",
        "      model.add(layers.Dense(1))\n",
        "      optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "      model.compile(loss='mse',optimizer=optimizer)\n",
        "      plot_model(model, to_file='CNNnew.png', show_shapes=True, show_layer_names=True)\n",
        "      model.summary()\n",
        "      mod = model.fit(X_train_data_reshaped, y_train , epochs=50, batch_size=2048, validation_data=(X_test_data_reshaped, y_test), verbose = 1,callbacks=[earlyStop])\n",
        "      y_test_hat = np.float64(scaler_y.inverse_transform(model.predict(X_test_data_reshaped)))\n",
        "      training_loss = mod.history['loss']\n",
        "      test_loss = mod.history['val_loss']\n",
        "      out = y_test_hat.reshape(y_test_hat.shape[0])\n",
        "      \n",
        "      if i == 0:\n",
        "        vol10['CNN']=pd.Series(out)\n",
        "      if i == 1:\n",
        "        vol30['CNN']=pd.Series(out)\n",
        "      if i == 2:\n",
        "        vol60['CNN']=pd.Series(out)\n",
        "      if i == 3:\n",
        "        ARIMA['CNN']=pd.Series(out)\n",
        "      if i == 4:\n",
        "        VXN['CNN']=pd.Series(out)                         \n",
        "                \n",
        "      # Create count of the number of epochs\n",
        "      epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "      # Visualize loss history\n",
        "      plt.plot(epoch_count, training_loss, 'r--')\n",
        "      plt.plot(epoch_count, test_loss, 'b-')\n",
        "      plt.legend(['Training Loss', 'Test Loss'])\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.show()\n",
        "\n",
        "      mse_test[i] = mean_squared_error(y, y_test_hat)\n",
        "      rmse_test[i] = math.sqrt(mse_test[i])\n",
        "      R2_test[i] = r2_score(y, y_test_hat)\n",
        "      mse_BSM_test[i] = mean_squared_error(y, BSM_test[:,i:i+1])\n",
        "      rmse_BSM_test[i] = math.sqrt(mse_BSM_test[i])\n",
        "      R2_BSM_test[i] = r2_score(y, BSM_test[:,i:i+1]) \n",
        "      model.save(modelList[k][i])\n",
        "      print('Model', i+1, 'Completed') \n",
        "\n",
        "    print( 'Saving Residuals')\n",
        "  \n",
        "    scoreList = [mse_test,rmse_test,R2_test]\n",
        "    scoreListBSM = [mse_BSM_test,rmse_BSM_test,R2_BSM_test]\n",
        "\n",
        "    for j in range(3):\n",
        "      filepath = Path('/content/drive/MyDrive/RUT_CNN/'+ scoreList_csv[k][j])  \n",
        "      filepath.parent.mkdir(parents=True, exist_ok=True) \n",
        "      df = pd.DataFrame( scoreList [j])\n",
        "      df = df.rename(columns={\"0\": \"vol10\", \"1\": \"vol30\", \"2\": \"vol60\", \"3\": \"ARIMA\", \"4\":\"VXN\"})\n",
        "      df.to_csv(filepath)\n",
        "    for j in range(3):\n",
        "      filepath = Path('/content/drive/MyDrive/RUT_CNN/'+ scoreListBSM_csv[k][j])  \n",
        "      filepath.parent.mkdir(parents=True, exist_ok=True) \n",
        "      df = pd.DataFrame( scoreListBSM [j])\n",
        "      df = df.rename(columns={\"0\": \"vol10\", \"1\": \"vol30\", \"2\": \"vol60\", \"3\": \"ARIMA\", \"4\":\"VXN\"})\n",
        "      df.to_csv(filepath)\n",
        "  return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining GridSearchCV parameters for CNN\n",
        "\n"
      ],
      "metadata": {
        "id": "3wFbvKUjA5U7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calling grid search\n",
        "def cnngridsearch():\n",
        "  space = dict()\n",
        "  space['batch_size'] = [512 , 1024 , 2048]\n",
        "  space['activation_function'] = ['LeakyReLU','tanh','relu']\n",
        "  space['optimizer'] = ['SGD', 'Adam']\n",
        "  search = GridSearchCV(cnn, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)\n",
        "  return search"
      ],
      "metadata": {
        "id": "RVsvoBjBCTtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSTM"
      ],
      "metadata": {
        "id": "WYnaeG8hn7Zj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67fQGS2ne8RS"
      },
      "outputs": [],
      "source": [
        "# defining the different models\n",
        "\n",
        "def lstm(x_train,x_test,y_train,y_test,y,vol_train,vol_test,scaler_y,optimalvalues):\n",
        "\n",
        "  from pathlib import Path  \n",
        "\n",
        "  for k in range(1):\n",
        "    mse_test,  mse_BSM_test = np.zeros(5), np.zeros(5)\n",
        "    rmse_test  , rmse_BSM_test = np. zeros (5) , np. zeros (5) \n",
        "    R2_test  , R2_BSM_test = np. zeros (5) , np. zeros (5) \n",
        "    diff_test ,  diff_BSM_test = np.zeros((len(x_test),5)), np.zeros((len(x_test),5))\n",
        "    modelList = [('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5', 'ARIMA RUT.h5', ' RVX RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' RVX RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' RVX RUT.h5 ' ),('vol10 RUT.h5','RUT SPX.h5','vol60 RUT.h5', 'ARIMA RUT.h5', ' RVX RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' RVX RUT.h5 ' ) ]\n",
        "    errorList_csv = [('diff_test_RUT_1.csv',  'diff_BSM_test1.csv' ),('diff_test_RUT_2.csv', 'diff_BSM_test2.csv' ),('diff_test_RUT_3.csv',  'diff_BSM_test3.csv' ),('diff_test_RUT_4.csv', 'diff_BSM_test4.csv' ),('diff_test_RUT_5.csv', 'diff_BSM_test5.csv' )]\n",
        "    scoreList_csv = [('mse_test1.csv','rmse_test1.csv','R2_test1.csv')]\n",
        "    scoreListBSM_csv = [('mse_BSM_test1.csv','rmse_BSM_test1.csv','R2_BSM_test1.csv')]\n",
        "    for i in range(5):\n",
        "\n",
        "      print('Fold', k+1, ',− Model', i+1)\n",
        "      X_train_sc = np.append(X_train, np.resize(vol_train[:,i], (len(vol_train),1)), axis = 1)\n",
        "      X_test_sc = np.append(X_test, np.resize(vol_test[:,i], (len(vol_test),1)), axis = 1)\n",
        "      \n",
        "      model = Sequential()\n",
        "      model.add(keras.layers.LSTM(100,return_sequences = True,input_shape=(X_train_sc.shape[1], 1)))\n",
        "      #model = Sequential()\n",
        "      #model.add(Reshape((TIME_PERIODS, num_sensors), input_shape=(input_shape,)))\n",
        "    # model.add(layers.LSTM(100,  activation='relu', input_shape=(TIME_PERIODS, num_sensors)))\n",
        "      model.add(layers.Bidirectional(layers.LSTM(64))) # making LSTM bidirectional\n",
        "      #model.add(layers.LSTM(units = 50,kernel_regularizer='l2',return_sequences = True,activation=keras.layers.LeakyReLU(alpha=0.01)))\n",
        "      model.add(layers.Dropout(0.1))\n",
        "      model.add(layers.Dense(1, activation='linear'))\n",
        "      model.compile(optimizer='adam', loss='mse')\n",
        "      model.summary()\n",
        "      plot_model(model, to_file='LSTMnew.png', show_shapes=True, show_layer_names=True)\n",
        "      mod = model.fit(X_train_sc, y_train , epochs=50, batch_size=2048, validation_data=(X_test_sc, y_test), verbose = 1,callbacks=[earlyStop])\n",
        "      y_test_hat = np.float64(scaler_y.inverse_transform(model.predict(X_test_sc)))\n",
        "      training_loss = mod.history['loss']\n",
        "      test_loss = mod.history['val_loss']\n",
        "      out = y_test_hat.reshape(y_test_hat.shape[0])\n",
        "      if i == 0:\n",
        "        vol10['LSTM']=pd.Series(out)\n",
        "      if i == 1:\n",
        "        vol30['LSTM']=pd.Series(out)\n",
        "      if i == 2:\n",
        "        vol60['LSTM']=pd.Series(out)\n",
        "      if i == 3:\n",
        "        ARIMA['LSTM']=pd.Series(out)\n",
        "      if i == 4:\n",
        "        RVX['LSTM']=pd.Series(out)                         \n",
        "                \n",
        "      # Create count of the number of epochs\n",
        "      epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "      # Visualize loss history\n",
        "      plt.plot(epoch_count, training_loss, 'r--')\n",
        "      plt.plot(epoch_count, test_loss, 'b-')\n",
        "      plt.legend(['Training Loss', 'Test Loss'])\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.show()\n",
        " \n",
        "   \n",
        "\n",
        "      mse_test[i] = mean_squared_error(y, y_test_hat)\n",
        "      rmse_test[i] = math.sqrt(mse_test[i])\n",
        "      R2_test[i] = r2_score(y, y_test_hat)\n",
        "      mse_BSM_test[i] = mean_squared_error(y, BSM_test[:,i:i+1])\n",
        "      rmse_BSM_test[i] = math.sqrt(mse_BSM_test[i])\n",
        "      R2_BSM_test[i] = r2_score(y, BSM_test[:,i:i+1]) \n",
        "      model.save(modelList[k][i])\n",
        "      print('Model', i+1, 'Completed') \n",
        "\n",
        "    print( 'Saving Residuals')\n",
        "    \n",
        "    scoreList = [mse_test,rmse_test,R2_test]\n",
        "    scoreListBSM = [mse_BSM_test,rmse_BSM_test,R2_BSM_test]\n",
        "    \n",
        "\n",
        "    for j in range(3):\n",
        "      filepath = Path('/content/drive/MyDrive/RUT_LSTM/'+ scoreList_csv[k][j])  \n",
        "      filepath.parent.mkdir(parents=True, exist_ok=True) \n",
        "      df = pd.DataFrame( scoreList [j])\n",
        "      df = df.rename(columns={\"0\": \"vol10\", \"1\": \"vol30\", \"2\": \"vol60\", \"3\": \"ARIMA\", \"4\":\"RVX\"})\n",
        "      df.to_csv(filepath)\n",
        "    for j in range(3):\n",
        "      filepath = Path('/content/drive/MyDrive/RUT_LSTM/'+ scoreListBSM_csv[k][j])  \n",
        "      filepath.parent.mkdir(parents=True, exist_ok=True) \n",
        "      df = pd.DataFrame( scoreListBSM [j])\n",
        "      df = df.rename(columns={\"0\": \"vol10\", \"1\": \"vol30\", \"2\": \"vol60\", \"3\": \"ARIMA\", \"4\":\"RVX\"})\n",
        "      df.to_csv(filepath)\n",
        "  return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining GridSearchCV parameters for LSTM\n",
        "\n"
      ],
      "metadata": {
        "id": "DwK56IREA6Ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calling grid search\n",
        "def lstmgridsearch():\n",
        "  space = dict()\n",
        "  space['batch_size'] = [512 , 1024 , 2048]\n",
        "  space['activation_function'] = ['LeakyReLU','tanh','relu']\n",
        "  space['optimizer'] = ['SGD', 'Adam']\n",
        "  search = GridSearchCV(lstm, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)\n",
        "  return search"
      ],
      "metadata": {
        "id": "EniOg-SZCNwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN-LSTM"
      ],
      "metadata": {
        "id": "U9wM5uC9n-pm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDgV9EwOs5N1"
      },
      "outputs": [],
      "source": [
        "# defining the different models\n",
        "\n",
        "def cnnlstm(x_train,x_test,y_train,y_test,y,vol_train,vol_test,scaler_y,optimalvalues):\n",
        "  from pathlib import Path  \n",
        "\n",
        "  for k in range(1):\n",
        "    mse_test,  mse_BSM_test = np.zeros(5), np.zeros(5)\n",
        "    rmse_test  , rmse_BSM_test = np. zeros (5) , np. zeros (5) \n",
        "    R2_test  , R2_BSM_test = np. zeros (5) , np. zeros (5) \n",
        "    diff_test ,  diff_BSM_test = np.zeros((len(x_test),5)), np.zeros((len(x_test),5))\n",
        "    modelList = [('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5', 'ARIMA RUT.h5', ' RVX RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' RVX RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' RVX RUT.h5 ' ),('vol10 RUT.h5','RUT SPX.h5','vol60 RUT.h5', 'ARIMA RUT.h5', ' RVX RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' RVX RUT.h5 ' ) ]\n",
        "    errorList_csv = [('diff_test_RUT_1.csv',  'diff_BSM_test1.csv' ),('diff_test_RUT_2.csv', 'diff_BSM_test2.csv' ),('diff_test_RUT_3.csv',  'diff_BSM_test3.csv' ),('diff_test_RUT_4.csv', 'diff_BSM_test4.csv' ),('diff_test_RUT_5.csv', 'diff_BSM_test5.csv' )]\n",
        "    scoreList_csv = [('mse_test1.csv','rmse_test1.csv','R2_test1.csv')]\n",
        "    scoreListBSM_csv = [('mse_BSM_test1.csv','rmse_BSM_test1.csv','R2_BSM_test1.csv')]\n",
        "    for i in range(5):\n",
        "      subsequences = 1\n",
        "      print('Fold', k+1, ',− Model', i+1)\n",
        "      X_train_sc = np.append(X_train, np.resize(vol_train[:,i], (len(vol_train),1)), axis = 1)\n",
        "      X_test_sc = np.append(X_test, np.resize(vol_test[:,i], (len(vol_test),1)), axis = 1)\n",
        "\n",
        "      subsequences = 1\n",
        "      X_train_size = X_train_sc.shape[0] # number of samples in train set\n",
        "      X_train_time_steps  = X_train_sc.shape[1]\n",
        "      train_timesteps = X_train_sc.shape[1]//subsequences  # number of features in train set\n",
        "      #print(X_train_time_steps)\n",
        "      input_dimension = 1  \n",
        " \n",
        "      X_train_data_reshaped = X_train_sc.reshape(X_train_size,X_train_time_steps,input_dimension)\n",
        "      #print(X_train_data_reshaped.shape)\n",
        "      X_train_data_reshaped = X_train_data_reshaped.reshape((X_train_data_reshaped.shape[0], subsequences, train_timesteps, 1))\n",
        "      \n",
        "      subsequences = 1\n",
        "      X_test_size = X_test_sc.shape[0] # number of samples in train set\n",
        "      X_test_time_steps  = X_test_sc.shape[1]\n",
        "      test_timesteps = X_test_sc.shape[1]//subsequences  # number of features in train set\n",
        "      #print(X_train_time_steps)\n",
        "      input_dimension = 1  \n",
        "      #print(X_test_sc.shape)             # each feature is represented by 1 number\n",
        "      X_test_data_reshaped = X_test_sc.reshape(X_test_size,X_test_time_steps,input_dimension)\n",
        "      #print(X_test_data_reshaped.shape)\n",
        "      X_test_data_reshaped = X_test_data_reshaped.reshape((X_test_data_reshaped.shape[0], subsequences, test_timesteps, 1))\n",
        "      model= Sequential()\n",
        "      model.add(keras.layers.TimeDistributed(keras.layers.Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, X_train_data_reshaped.shape[2], X_train_data_reshaped.shape[3])))\n",
        "      #model.add(keras.layers.TimeDistributed(keras.layers.MaxPooling1D(pool_size=2)))\n",
        "      model.add(keras.layers.TimeDistributed(keras.layers.Flatten()))\n",
        "      model.add(keras.layers.LSTM(50, activation='elu'))\n",
        "      model.add(keras.layers.Dense(1))\n",
        "      model.compile(optimizer='adam', loss='mse')\n",
        "      plot_model(model, to_file='CNNLSTMnew.png', show_shapes=True, show_layer_names=True)\n",
        "      model.summary()\n",
        "      mod = model.fit(X_train_data_reshaped, y_train , epochs=50, batch_size=2048, validation_data=(X_test_data_reshaped, y_test), verbose = 1,callbacks=[earlyStop])\n",
        "      y_test_hat = np.float64(scaler_y.inverse_transform(model.predict(X_test_data_reshaped)))\n",
        "      training_loss = mod.history['loss']\n",
        "      test_loss = mod.history['val_loss']\n",
        "      out = y_test_hat.reshape(y_test_hat.shape[0])\n",
        "      if i == 0:\n",
        "        vol10['CNNLSTM']=pd.Series(out)\n",
        "      if i == 1:\n",
        "        vol30['CNNLSTM']=pd.Series(out)\n",
        "      if i == 2:\n",
        "        vol60['CNNLSTM']=pd.Series(out)\n",
        "      if i == 3:\n",
        "        ARIMA['CNNLSTM']=pd.Series(out)\n",
        "      if i == 4:\n",
        "        RVX['CNNLSTM']=pd.Series(out)                         \n",
        "                \n",
        "      # Create count of the number of epochs\n",
        "      epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "      # Visualize loss history\n",
        "      plt.plot(epoch_count, training_loss, 'r--')\n",
        "      plt.plot(epoch_count, test_loss, 'b-')\n",
        "      plt.legend(['Training Loss', 'Test Loss'])\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.show()\n",
        "\n",
        "      mse_test[i] = mean_squared_error(y, y_test_hat)\n",
        "      rmse_test[i] = math.sqrt(mse_test[i])\n",
        "      R2_test[i] = r2_score(y, y_test_hat)\n",
        "      mse_BSM_test[i] = mean_squared_error(y, BSM_test[:,i:i+1])\n",
        "      rmse_BSM_test[i] = math.sqrt(mse_BSM_test[i])\n",
        "      R2_BSM_test[i] = r2_score(y, BSM_test[:,i:i+1]) \n",
        "      model.save(modelList[k][i])\n",
        "      print('Model', i+1, 'Completed') \n",
        "\n",
        "    print( 'Saving Residuals')\n",
        "    \n",
        "    scoreList = [mse_test,rmse_test,R2_test]\n",
        "    scoreListBSM = [mse_BSM_test,rmse_BSM_test,R2_BSM_test]\n",
        "\n",
        "\n",
        "    for j in range(3):\n",
        "      filepath = Path('/content/drive/MyDrive/RUT_CNNLSTM/'+ scoreList_csv[k][j])  \n",
        "      filepath.parent.mkdir(parents=True, exist_ok=True) \n",
        "      df = pd.DataFrame( scoreList [j])\n",
        "      df = df.rename(columns={\"0\": \"vol10\", \"1\": \"vol30\", \"2\": \"vol60\", \"3\": \"ARIMA\", \"4\":\"RVX\"})\n",
        "      df.to_csv(filepath)\n",
        "    for j in range(3):\n",
        "      filepath = Path('/content/drive/MyDrive/RUT_CNNLSTM/'+ scoreListBSM_csv[k][j])  \n",
        "      filepath.parent.mkdir(parents=True, exist_ok=True) \n",
        "      df = pd.DataFrame( scoreListBSM [j])\n",
        "      df = df.rename(columns={\"0\": \"vol10\", \"1\": \"vol30\", \"2\": \"vol60\", \"3\": \"ARIMA\", \"4\":\"RVX\"})\n",
        "      df.to_csv(filepath)\n",
        "  return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining GridSearchCV parameters for CNN-LSTM\n",
        "\n"
      ],
      "metadata": {
        "id": "52bkVkeaA6qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calling grid search\n",
        "def cnnlstmgridsearch():\n",
        "  space = dict()\n",
        "  space['batch_size'] = [512 , 1024 , 2048]\n",
        "  space['activation_function'] = ['LeakyReLU','tanh','relu']\n",
        "  space['optimizer'] = ['SGD', 'Adam']\n",
        "  search = GridSearchCV(cnnlstm, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)\n",
        "  return search"
      ],
      "metadata": {
        "id": "em3V2_TDCFYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DUAL HYBRID\n"
      ],
      "metadata": {
        "id": "SSxTPOcFoGAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bsm_call(S, K, T, r, sigma):\n",
        "    a = (np.log(S/K) + (r + sigma**2/2)*T) / (sigma*np.sqrt(T))\n",
        "    b = a - sigma * np.sqrt(T)\n",
        "    return S * N(a) - K * np.exp(-r*T)* N(b)\n",
        "\n",
        "def merton_jump_paths(S, T, r, sigma,  lam, m, v, steps, Npaths):\n",
        "    size=(steps,Npaths)\n",
        "    dt = T/steps \n",
        "    poi_rv = np.multiply(np.random.poisson( lam*dt, size=size),\n",
        "                         np.random.normal(m,v, size=size)).cumsum(axis=0)\n",
        "    geo = np.cumsum(((r -  sigma**2/2 -lam*(m  + v**2*0.5))*dt +\\\n",
        "                              sigma*np.sqrt(dt) * \\\n",
        "                              np.random.normal(size=size)), axis=0)\n",
        "    \n",
        "    return np.exp(geo+poi_rv)*S\n",
        "\n",
        "\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize_scalar   \n",
        "N = norm.cdf\n",
        "\n",
        "\n",
        "def mjdcall(S, K, T, r, sigma, m , v, lam):\n",
        "    p = 0\n",
        "    for k in range(40):\n",
        "        rk = r - lam*(m-1) + (k*np.log(m) ) / T\n",
        "        sigmak = np.sqrt( sigma**2 + (k* v** 2) / T)\n",
        "        Kfact = np.math.factorial(k)\n",
        "        p += (np.exp(-m*lam*T) * (m*lam*T)**k / (Kfact))  * bsm_call(S, K, T, rk, sigmak)\n",
        "    \n",
        "    return p \n",
        "    \n",
        "def optimal_params(x, a,mkt_prices, strikes):\n",
        "    T = a['T'].values[i]\n",
        "    S = a['Close'].values[i]\n",
        "    r = a['riskefreerate'].values[i]\n",
        "    price = a['price'].values[i] \n",
        "    candidate_prices = mjdcall(S, strikes, T, r,\n",
        "                                        sigma=x[0], m= x[1] ,\n",
        "                                        v=x[2],lam= x[3])\n",
        "    return np.linalg.norm([mkt_prices - candidate_prices], 2)\n",
        "\n",
        "def optimize_mjd(a,price,strikes):\n",
        "  \n",
        "      \n",
        "  x0 = [0.15, 1, 0.1, 1] \n",
        "  bounds = ((0.01, np.inf) , (0.01, 2), (1e-5, np.inf) , (0, 5)) #bounds as described above\n",
        "\n",
        "  res = minimize(optimal_params, method='SLSQP',  x0=x0, args=(a,price, strikes),bounds = bounds, tol=10e-4)\n",
        "  sigt = res.x[0]\n",
        "  mt = res.x[1]\n",
        "  vt = res.x[2]\n",
        "  lamt = res.x[3]\n",
        "  return sigt,mt,vt,lamt\n",
        "\n",
        "def dualmjd(a):\n",
        "  deviation = []\n",
        "  for i in range(len(a)):\n",
        "\n",
        "    T = a['T'].values[i]\n",
        "    S = a['Close'].values[i]\n",
        "    r = a['riskefreerate'].values[i]\n",
        "    price = a['price'].values[i]\n",
        "    \n",
        "    strike = a['strike_price'].values[i]\n",
        "    sigt,mt,vt,lamt = optimize_mjd(a,price, strike)\n",
        "    temp = mjdcall(S,strike,T,r,sigt,mt,vt,lamt)\n",
        "\n",
        "    \n",
        "    deviation.append(temp-price) \n",
        "    print(i+1)\n",
        "  return deviation\n",
        "def dualhybrid(df1,optimalvalues):\n",
        "  df2 = df1.drop_duplicates(subset=[\"strike_price\"], keep='first')\n",
        "  x =dualmjd(df2)\n",
        "  x = pd.DataFrame (x, columns = ['err'])\n",
        "  x['date'] = df2.index\n",
        "  x = x.set_index('date')\n",
        "  df2 = df1.merge(x,left_index = True, right_index = True,how='left')\n",
        "  print(df2)\n",
        "  df2.dropna(inplace=True)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(df2[['T','M', 'riskefreerate' ,'rate','err','vol10','vol30','vol60','ARIMA','RVX','BSM (vol10)','BSM (vol30)','BSM (vol60)','BSM (Arima)','BSM (RVX)']].values, df2[['price']].values , test_size = 0.10, random_state = 110493)\n",
        "  scaler_x = RobustScaler()\n",
        "  scaler_y = RobustScaler()\n",
        "  X_train_sc = scaler_x.fit_transform(X_train)\n",
        "  X_test_sc = scaler_x.fit_transform(X_test)\n",
        "  y_train_sc = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
        "  y_test_sc = scaler_y.fit_transform(y_test.reshape(-1, 1))\n",
        "  print(X_train)\n",
        "  print(X_train_sc)\n",
        "  BSM_train = np.resize(X_train[:,10:15], (len(X_train),5))\n",
        "  BSM_test = np.resize(X_test[:,10:15], (len(X_test),5)) \n",
        "  vol_train = np.resize(X_train_sc[:,5:10], (len(X_train_sc),5))\n",
        "  X_train = X_train_sc[: ,0:5]\n",
        "  vol_test = np.resize(X_test_sc[: ,5:10] , (len(X_test_sc) ,5))\n",
        "  X_test = X_test_sc[:,0:5]\n",
        "  from pathlib import Path  \n",
        "\n",
        "  for k in range(1):\n",
        "    mse_test,  mse_BSM_test = np.zeros(5), np.zeros(5)\n",
        "    rmse_test  , rmse_BSM_test = np. zeros (5) , np. zeros (5) \n",
        "    R2_test  , R2_BSM_test = np. zeros (5) , np. zeros (5) \n",
        "    diff_test ,  diff_BSM_test = np.zeros((len(X_test),5)), np.zeros((len(X_test),5))\n",
        "    modelList = [('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5', 'ARIMA RUT.h5', ' RVX RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' RVX RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' RVX RUT.h5 ' ),('vol10 RUT.h5','RUT SPX.h5','vol60 RUT.h5', 'ARIMA RUT.h5', ' RVX RUT.h5 ' ),('vol10 RUT.h5','vol30 RUT.h5','vol60 RUT.h5','ARIMA RUT.h5', ' RVX RUT.h5 ' ) ]\n",
        "    errorList_csv = [('diff_test_RUT_1.csv',  'diff_BSM_test1.csv' ),('diff_test_RUT_2.csv', 'diff_BSM_test2.csv' ),('diff_test_RUT_3.csv',  'diff_BSM_test3.csv' ),('diff_test_RUT_4.csv', 'diff_BSM_test4.csv' ),('diff_test_RUT_5.csv', 'diff_BSM_test5.csv' )]\n",
        "    scoreList_csv = [('mse_test1.csv','rmse_test1.csv','R2_test1.csv')]\n",
        "    scoreListBSM_csv = [('mse_BSM_test1.csv','rmse_BSM_test1.csv','R2_BSM_test1.csv')]\n",
        "    for i in range(5):\n",
        "      subsequences = 1\n",
        "      print('Fold', k+1, ',− Model', i+1)\n",
        "      X_train = np.append(X_train_sc, np.resize(vol_train[:,i], (len(vol_train),1)), axis = 1)\n",
        "      X_test = np.append(X_test_sc, np.resize(vol_test[:,i], (len(vol_test),1)), axis = 1)\n",
        "\n",
        "      subsequences = 1\n",
        "      X_train_size = X_train.shape[0] # number of samples in train set\n",
        "      X_train_time_steps  = X_train.shape[1]\n",
        "      train_timesteps = X_train.shape[1]//subsequences  # number of features in train set\n",
        "      #print(X_train_time_steps)\n",
        "      input_dimension = 1  \n",
        " \n",
        "      X_train_data_reshaped = X_train.reshape(X_train_size,X_train_time_steps,input_dimension)\n",
        "      #print(X_train_data_reshaped.shape)\n",
        "      X_train_data_reshaped = X_train_data_reshaped.reshape((X_train_data_reshaped.shape[0], subsequences, train_timesteps, 1))\n",
        "      \n",
        "      subsequences = 1\n",
        "      X_test_size = X_test.shape[0] # number of samples in train set\n",
        "      X_test_time_steps  = X_test.shape[1]\n",
        "      test_timesteps = X_test.shape[1]//subsequences  # number of features in train set\n",
        "      #print(X_train_time_steps)\n",
        "      input_dimension = 1  \n",
        "      #print(X_test_sc.shape)             # each feature is represented by 1 number\n",
        "      X_test_data_reshaped = X_test.reshape(X_test_size,X_test_time_steps,input_dimension)\n",
        "      #print(X_test_data_reshaped.shape)\n",
        "      X_test_data_reshaped = X_test_data_reshaped.reshape((X_test_data_reshaped.shape[0], subsequences, test_timesteps, 1))\n",
        "      model= Sequential()\n",
        "      model.add(keras.layers.TimeDistributed(keras.layers.Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, X_train_data_reshaped.shape[2], X_train_data_reshaped.shape[3])))\n",
        "      #model.add(keras.layers.TimeDistributed(keras.layers.MaxPooling1D(pool_size=2)))\n",
        "      model.add(keras.layers.TimeDistributed(keras.layers.Flatten()))\n",
        "      model.add(keras.layers.LSTM(50, activation='elu'))\n",
        "      model.add(keras.layers.Dense(1))\n",
        "      model.compile(optimizer='adam', loss='mse')\n",
        "      model.summary()\n",
        "      mod = model.fit(X_train_data_reshaped, y_train_sc , epochs=50, batch_size=2048, validation_data=(X_test_data_reshaped, y_test_sc), verbose = 1,callbacks=[earlyStop])\n",
        "      y_test_hat = np.float64(scaler_y.inverse_transform(model.predict(X_test_data_reshaped)))\n",
        "      training_loss = mod.history['loss']\n",
        "      test_loss = mod.history['val_loss']\n",
        "      out = y_test_hat.reshape(y_test_hat.shape[0])\n",
        "      if i == 0:\n",
        "        vol10['MJD_CNNLSTM']=pd.Series(out)\n",
        "      if i == 1:\n",
        "        vol30['MJD_CNNLSTM']=pd.Series(out)\n",
        "      if i == 2:\n",
        "        vol60['MJD_CNNLSTM']=pd.Series(out)\n",
        "      if i == 3:\n",
        "        ARIMA['MJD_CNNLSTM']=pd.Series(out)\n",
        "      if i == 4:\n",
        "        RVX['MJD_CNNLSTM']=pd.Series(out)                         \n",
        "                \n",
        "      # Create count of the number of epochs\n",
        "      epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "      # Visualize loss history\n",
        "      plt.plot(epoch_count, training_loss, 'r--')\n",
        "      plt.plot(epoch_count, test_loss, 'b-')\n",
        "      plt.legend(['Training Loss', 'Test Loss'])\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.show()\n",
        "\n",
        "      mse_test[i] = mean_squared_error(y_test, y_test_hat)\n",
        "      rmse_test[i] = math.sqrt(mse_test[i])\n",
        "      R2_test[i] = r2_score(y_test, y_test_hat)\n",
        "      mse_BSM_test[i] = mean_squared_error(y_test, BSM_test[:,i:i+1])\n",
        "      rmse_BSM_test[i] = math.sqrt(mse_BSM_test[i])\n",
        "      R2_BSM_test[i] = r2_score(y_test, BSM_test[:,i:i+1]) \n",
        "      model.save(modelList[k][i])\n",
        "      print('Model', i+1, 'Completed') \n",
        "\n",
        "    print( 'Saving Residuals')\n",
        "    \n",
        "    scoreList = [mse_test,rmse_test,R2_test]\n",
        "    scoreListBSM = [mse_BSM_test,rmse_BSM_test,R2_BSM_test]\n",
        "\n",
        "    for j in range(3):\n",
        "      filepath = Path('/content/drive/MyDrive/RUT_MJD_CNNLSTM/'+ scoreList_csv[k][j])  \n",
        "      filepath.parent.mkdir(parents=True, exist_ok=True) \n",
        "      df = pd.DataFrame( scoreList [j])\n",
        "      df = df.rename(columns={\"0\": \"vol10\", \"1\": \"vol30\", \"2\": \"vol60\", \"3\": \"ARIMA\", \"4\":\"RVX\"})\n",
        "      df.to_csv(filepath)\n",
        "    for j in range(3):\n",
        "      filepath = Path('/content/drive/MyDrive/RUT_MJD_CNNLSTM/'+ scoreListBSM_csv[k][j])  \n",
        "      filepath.parent.mkdir(parents=True, exist_ok=True) \n",
        "      df = pd.DataFrame( scoreListBSM [j])\n",
        "      df = df.rename(columns={\"0\": \"vol10\", \"1\": \"vol30\", \"2\": \"vol60\", \"3\": \"ARIMA\", \"4\":\"RVX\"})\n",
        "      df.to_csv(filepath)\n",
        "  return\n",
        " "
      ],
      "metadata": {
        "id": "fOPCFGi5E74H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining GridSearchCV parameters for Jump-Diffusion CNN-LSTM\n",
        "\n"
      ],
      "metadata": {
        "id": "IuRjsH4XA9Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calling grid search\n",
        "def dualmjdgridsearch():\n",
        "  space = dict()\n",
        "  space['batch_size'] = [512 , 1024 , 2048]\n",
        "  space['activation_function'] = ['LeakyReLU','tanh','relu']\n",
        "  space['optimizer'] = ['SGD', 'Adam']\n",
        "  search = GridSearchCV(dualmjd, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)\n",
        "  return search"
      ],
      "metadata": {
        "id": "RBKVIDDAB_pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExoGQ7-GsqeE"
      },
      "source": [
        "## RUT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPLlp7RVsqNK"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/DATA/RUT.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAMywqNNtGRd"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=['secid','symbol','cp_flag','optionid','am_set_flag'])\n",
        "df['strike_price'] = df['strike_price']/1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sstE55nvtdbh"
      },
      "outputs": [],
      "source": [
        "df['date'] = pd.to_datetime(df['date'], format = '%Y%m%d')\n",
        "df['exdate'] = pd.to_datetime(df['exdate'], format = '%Y%m%d')\n",
        "df['T'] = ((df['exdate'] - df['date']).dt.days)/365 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAF90h4IeBZ_"
      },
      "outputs": [],
      "source": [
        "pip install  yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7wzv5VUd77Y"
      },
      "outputs": [],
      "source": [
        "pip install yfinance --upgrade --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fINH_GgceKZj"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6Ufa3igvzu5"
      },
      "outputs": [],
      "source": [
        "c = yf.download(\"^RUT\", start='2012-01-01', end='2022-01-01')\n",
        "c = c.drop(columns=['High','Open','Low','Adj Close','Volume'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKD6SfuXwdM1"
      },
      "outputs": [],
      "source": [
        "c.dropna(inplace=True)\n",
        "df.dropna(inplace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra4FoE9kxEGu"
      },
      "outputs": [],
      "source": [
        "df = df.set_index('date')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efCGl7NqwE39"
      },
      "outputs": [],
      "source": [
        "df1 = df.merge(c , left_index =True, right_index = True, how = 'left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOvKdwvDwSON"
      },
      "outputs": [],
      "source": [
        "df1['price'] = ((df['best_bid'] + df['best_offer']))/2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdSojIyaxrqh"
      },
      "outputs": [],
      "source": [
        "riskfree = pd.read_csv('/content/drive/MyDrive/DATA/10YM.csv',parse_dates = True, index_col = 'DATE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULPOyh5H4yV1"
      },
      "outputs": [],
      "source": [
        "riskfree.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEe5dtSxyOU2"
      },
      "outputs": [],
      "source": [
        "\n",
        "riskfree['DGS10'] = riskfree['DGS10'].replace(\".\",\"\", regex=False)\n",
        "\n",
        "riskfree['DGS10'] = riskfree['DGS10'].replace(\"\",np.nan, regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eoe0auWM5n0Q"
      },
      "outputs": [],
      "source": [
        "riskfree.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxGa52qDx0Va"
      },
      "outputs": [],
      "source": [
        "riskfree = riskfree.rename(columns={\"DGS10\": \"riskefreerate\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPuiWBDzyTe6"
      },
      "outputs": [],
      "source": [
        "df1 = df1.merge(riskfree , left_index =True, right_index = True, how = 'left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzPf5ywP4cDx"
      },
      "outputs": [],
      "source": [
        "df1.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_HbxV-_4b-h"
      },
      "outputs": [],
      "source": [
        "df1['riskefreerate'] =df1['riskefreerate'].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIUojf0C0Bf8"
      },
      "outputs": [],
      "source": [
        "volindex = pd.read_csv('/content/drive/MyDrive/DATA/RVX.csv', parse_dates = True, index_col = 'Date')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4U-JuIs0PZ2"
      },
      "outputs": [],
      "source": [
        "df1 = df1.merge(volindex , left_index =True, right_index = True, how = 'left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEe4He4D0Rhz"
      },
      "outputs": [],
      "source": [
        "df1 = df1.rename(columns={\"RVX\": \"RVX\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IQYy0dp6boS"
      },
      "outputs": [],
      "source": [
        "df1['RVX'] = df1['RVX'].astype(float)\n",
        "df1['RVX'] = df1['RVX']/100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECgcK50J3noc"
      },
      "outputs": [],
      "source": [
        "df1['riskefreerate'] =df1['riskefreerate']/100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw4lKwj663SS"
      },
      "outputs": [],
      "source": [
        "df1['Close'] = df1['Close'].replace(\",\",\"\", regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prq6d88J7AI5"
      },
      "outputs": [],
      "source": [
        "df1['Close'] = df1['Close'].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtHzBpYI8AO1"
      },
      "outputs": [],
      "source": [
        "df1.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdUwcrgMeMn3"
      },
      "outputs": [],
      "source": [
        "c = yf.Ticker(\"^RVX\")\n",
        "c.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5cK0EzHyDSn"
      },
      "outputs": [],
      "source": [
        "c = yf.download(\"^RUT\", start='2002-01-01', end='2022-01-01')\n",
        "c = c.drop(columns=['High','Open','Low','Adj Close','Volume'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-j1vs7pTkE7"
      },
      "outputs": [],
      "source": [
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVmDZQpeyXF0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii9a97egy8px"
      },
      "outputs": [],
      "source": [
        "c['return'] = c['Close'].pct_change()\n",
        "c.dropna(inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZgcFV4TV7Sr"
      },
      "outputs": [],
      "source": [
        "c = c.drop(columns=['Close'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRJK3DXoEmJx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Historical Volatility (Rolling)\n",
        "vol = pd.DataFrame(columns = ['vol10','vol30','vol60'])\n",
        "vol['vol10'] = c['return'].rolling(window = 10).std()*np.sqrt(252)\n",
        "vol['vol30'] = c['return'].rolling(window = 30).std()*np.sqrt(252)\n",
        "vol['vol60'] = c['return'].rolling(window = 60).std()*np.sqrt(252)\n",
        "vol.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-6LDGIyujZ7"
      },
      "outputs": [],
      "source": [
        "import datetime as dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8g_oYjduams"
      },
      "outputs": [],
      "source": [
        "\n",
        "start = c.index.searchsorted(dt.datetime(2012, 1, 1))\n",
        "\n",
        "end = c.index.searchsorted(dt.datetime(2021, 12, 31))\n",
        "\n",
        "y = c.iloc[start:end]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgwqwVpbuvEW"
      },
      "outputs": [],
      "source": [
        "\n",
        "start = c.index.searchsorted(dt.datetime(2002, 1, 1))\n",
        "\n",
        "end = c.index.searchsorted(dt.datetime(2011, 12, 31))\n",
        "\n",
        "x = c.iloc[start:end]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQzGeHrUVY5x"
      },
      "outputs": [],
      "source": [
        "# evaluate an ARIMA model using a walk-forward validation\n",
        "from pandas import read_csv\n",
        "from pandas import datetime\n",
        "from matplotlib import pyplot\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "# load dataset\n",
        "\n",
        "# split into train and test sets\n",
        "X = x.values\n",
        "X = X*100\n",
        "train = X[:]\n",
        "Y = y.values\n",
        "test = Y[:] \n",
        "history = [x for x in train]\n",
        "predictions = list()\n",
        "# walk-forward validation\n",
        "i = 0\n",
        "for t in range(len(test)):\n",
        "    \n",
        "\tmodel = ARIMA(history, order=(5,1,0))\n",
        "\tmodel_fit = model.fit()\n",
        "\toutput = model_fit.forecast()\n",
        "\tyhat = output[0]\n",
        "\tpredictions.append(yhat)\n",
        "  #preditctions\n",
        "\tobs = test[t]\n",
        "\thistory.append(obs)\n",
        "\tprint(t)\n",
        "\n",
        "\n",
        "# evaluate forecasts\n",
        "df = pd.DataFrame(predictions)\n",
        "df = df.astype(float)\n",
        "\n",
        "df = abs(df)\n",
        "df = np.sqrt(df) \n",
        "df = df.set_index(y.index)\n",
        "vol['ARIMA']=df\n",
        "#rmse = sqrt(mean_squared_error(test, predictions))\n",
        "#print('Test RMSE: %.3f' % rmse)\n",
        "# plot forecasts against actual outcomes\n",
        "#pyplot.plot(test)\n",
        "#pyplot.plot(predictions, col)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2ZJVu1ZfB7o"
      },
      "outputs": [],
      "source": [
        "vol.dropna(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jJDGKdl-aRl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HnGHohT9-aA"
      },
      "outputs": [],
      "source": [
        "df1 = df1.merge(vol, left_index=True, right_index=True, how = 'left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiJ1vz-1LRVD"
      },
      "outputs": [],
      "source": [
        "df1.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie1nbccK-Y3x"
      },
      "outputs": [],
      "source": [
        "df1 = df1.drop(columns=['issuer','ticker','index_flag','exercise_style','exdate'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czYr5_ul_cKD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Continous Dividends (q)\n",
        "q = pd.read_csv('/content/drive/MyDrive/DATA/RUTdiv.csv', parse_dates= True )\n",
        "q['date'] = pd.to_datetime(q['date'], format = '%Y%m%d')\n",
        "q = q.set_index('date')\n",
        "q['rate']=q['rate']/100\n",
        "\n",
        "q = pd.DataFrame(q['rate'])\n",
        "df1 = df1.merge(q, left_index=True, right_index=True, how='left')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th7lJ01yLgQC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50o58en8TQBV"
      },
      "outputs": [],
      "source": [
        "df1['BSM (vol10)'] = (bsmcall(df1['Close'],df1['strike_price'],df1['T'],df1['riskefreerate'],df1['rate'],df1['vol10'])) / df1['strike_price']\n",
        "df1['BSM (vol30)'] = (bsmcall(df1['Close'],df1['strike_price'],df1['T'],df1['riskefreerate'],df1['rate'],df1['vol30'])) / df1['strike_price']\n",
        "df1['BSM (vol60)'] = (bsmcall(df1['Close'],df1['strike_price'],df1['T'],df1['riskefreerate'],df1['rate'],df1['vol60'])) / df1['strike_price']\n",
        "df1['BSM (Arima)'] = (bsmcall(df1['Close'],df1['strike_price'],df1['T'],df1['riskefreerate'],df1['rate'],df1['ARIMA'])) / df1['strike_price']\n",
        "df1['BSM (RVX)'] = (bsmcall(df1['Close'],df1['strike_price'],df1['T'],df1['riskefreerate'],df1['rate'],df1['RVX'])) / df1['strike_price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lSAFw5BU9wI"
      },
      "outputs": [],
      "source": [
        "df1['M'] = df1['Close']/df1['strike_price']\n",
        "df1 = df1[(df1['M'] <= 1.5)] \n",
        "df1 = df1[(df1['M'] >= 0.8)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbj3c97EiSjL"
      },
      "outputs": [],
      "source": [
        "df1['price'] = df1['price'] / df1['strike_price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eZKPgip05AI"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df1[['T','M', 'riskefreerate' ,'rate','RVX','vol10','vol30','vol60','ARIMA','BSM (vol10)','BSM (vol30)','BSM (vol60)','BSM (Arima)','BSM (RVX)','strike_price']].values, df1[['price']].values , test_size = 0.10, random_state = 110493)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAWezGIX0-XW"
      },
      "outputs": [],
      "source": [
        "scaler_x = RobustScaler()\n",
        "scaler_y = RobustScaler()\n",
        "X_train_sc = scaler_x.fit_transform(X_train)\n",
        "X_test_sc = scaler_x.fit_transform(X_test)\n",
        "y_train_sc = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_sc = scaler_y.fit_transform(y_test.reshape(-1, 1))\n",
        "BSM_train = np.resize(X_train[:,9:14], (len(X_train),5))\n",
        "X_train = X_train[: ,0:5]\n",
        "strike_test = np.resize(X_test[:,14:15], (len(X_test),1)) \n",
        "BSM_test = np.resize(X_test[:,9:14], (len(X_test),5)) \n",
        "X_test =X_test[:,0:5]\n",
        "vol_train = np.resize(X_train_sc[:,4:9], (len(X_train_sc),5))\n",
        "X_train = X_train_sc[: ,0:4]\n",
        "vol_test = np.resize(X_test_sc[: ,4:9] , (len(X_test_sc) ,5))\n",
        "X_test = X_test_sc[:,0:4]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vol10 = pd.DataFrame()\n",
        "vol30 = pd.DataFrame()\n",
        "vol60 = pd.DataFrame()\n",
        "ARIMA = pd.DataFrame()\n",
        "RVX = pd.DataFrame()"
      ],
      "metadata": {
        "id": "-XT3D386m9f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F03XJW4TjWT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caling gridsearch and respective model.\n"
      ],
      "metadata": {
        "id": "OZt1-i4yDX8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimalvalues = mlpgridsearch()"
      ],
      "metadata": {
        "id": "YHMufPC1DcEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS6nR5D-7Q-0"
      },
      "outputs": [],
      "source": [
        "mlp(X_train,X_test,y_train_sc,y_test_sc,y_test,vol_train,vol_test,scaler_y,BSM_train,BSM_test,optimalvalues)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimalvalues = dualmjdgridsearch()"
      ],
      "metadata": {
        "id": "krXPhZCjDr5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dualhybrid(df1,optimalvalues)"
      ],
      "metadata": {
        "id": "gL8zUu94kLfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimalvalues = cnngridsearch()"
      ],
      "metadata": {
        "id": "EAl28xuFDzei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xIv4vRQCWnv"
      },
      "outputs": [],
      "source": [
        "cnn(X_train_sc,X_test_sc,y_train_sc,y_test_sc,y_test,vol_train,vol_test,scaler_y,optimalvalues)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimalvalues = lstmgridsearch()"
      ],
      "metadata": {
        "id": "67Pe7Jc1D4Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQGoacCMGzdl"
      },
      "outputs": [],
      "source": [
        "lstm(X_train_sc,X_test_sc,y_train_sc,y_test_sc,y_test,vol_train,vol_test,scaler_y,optimalvalues)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimalvalues = cnnlstmgridsearch()"
      ],
      "metadata": {
        "id": "A6fyVj5DD767"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4pr8TZiJQ6E"
      },
      "outputs": [],
      "source": [
        "\n",
        "cnnlstm(X_train_sc,X_test_sc,y_train_sc,y_test_sc,y_test,vol_train,vol_test,scaler_y,optimalvalues)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5hxuSqpznq9_",
        "WYnaeG8hn7Zj",
        "U9wM5uC9n-pm",
        "SSxTPOcFoGAc",
        "tJWaHtbcfCBw",
        "w4rn9g-ssnjZ"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}